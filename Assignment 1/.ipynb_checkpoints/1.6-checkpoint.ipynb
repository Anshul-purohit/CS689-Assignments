{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb81d695",
   "metadata": {},
   "source": [
    "# Question : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cb225",
   "metadata": {},
   "source": [
    "## Tokenization Comparison Report\n",
    "### 1. Introduction\n",
    "- Tokenization is a pivotal step in natural language processing, influencing the effectiveness of downstream tasks. This report assesses the performance of five tokenization methods for Hindi sentences: Unigram, Byte Pair Encoding (BPE), mBERT, IndicBERT, and Whitespace Tokenizer.\n",
    "### 2. Tokenization Methods\n",
    "- **Unigram Tokenizer:** Basic tokenization based on individual word units.\n",
    "- **Byte Pair Encoding (BPE) Tokenizer:** Merges frequent character pairs iteratively to form tokens.\n",
    "- **mBERT Tokenizer:** Utilizes multilingual BERT embeddings for tokenization.\n",
    "- **IndicBERT Tokenizer:** A BERT-based tokenizer specifically designed for Indic languages.\n",
    "- **Whitespace Tokenizer:** Splits sentences based on whitespace.\n",
    "### 3. Evaluation Criteria\n",
    "- **Precision:** The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "- **Recall:** The ratio of correctly predicted positive observations to the total actual positives.\n",
    "- **F1 Score:** The harmonic mean of precision and recall.\n",
    "### 4. Data Used\n",
    "- The evaluation is based on a dataset of manually tokenized Hindi sentences.\n",
    "### 5. Results\n",
    "\n",
    "#### Unigram Tokenizer:\n",
    "- Precision: 8.28%\n",
    "- Recall: 25.62%\n",
    "- F1 Score: 12.52%\n",
    "\n",
    "#### BPE Tokenizer:\n",
    "- Precision: 6.80%\n",
    "- Recall: 21.94%\n",
    "- F1 Score: 10.38%\n",
    "\n",
    "#### mBERT Tokenizer:\n",
    "- Precision: 12.74%\n",
    "- Recall: 30.72%\n",
    "- F1 Score: 18.01%\n",
    "\n",
    "#### IndicBERT Tokenizer:\n",
    "- Precision: 11.97%\n",
    "- Recall: 30.72%\n",
    "- F1 Score: 17.22%\n",
    "\n",
    "#### Whitespace Tokenizer:\n",
    "- Precision: 21.18%\n",
    "- Recall: 39.75%\n",
    "- F1 Score: 27.63%\n",
    "\n",
    "\n",
    "### Tokenization Summary\n",
    "- The evaluation of various tokenization models for Hindi sentences reveals significant challenges, particularly related to Unicode encoding. The tokenizers assessed, including Unigram, Byte Pair Encoding (BPE), mBERT, IndicBERT, and Whitespace Tokenizer, exhibit limitations in accurately representing Hindi language. The observed metrics, including precision, recall, and F1 Score, indicate suboptimal performance across the board.\n",
    "- The available evaluation metrics do not fully capture the challenges posed by the complexities of Sandhi and Samas. As a result, the tokenizers may inadequately represent the richness and variety of Hindi words, particularly when these structures are present, leading to a decline in tokenization accuracy.\n",
    "- The models, including mBERT and IndicBERT, designed for multilingual scenarios, may struggle to handle the intricacies of Sandhi and Samas in Hindi. There is a pressing need for the development of language-specific tokenization strategies to improve accuracy and fidelity when these linguistic structures are encountered.\n",
    "\n",
    "### Conclusion\n",
    "- In conclusion, the tokenization models' suboptimal performance on Hindi sentences, exacerbated by Unicode encoding issues and challenges related to Sandhi and Samas structures, underscores the critical need for advancements in tokenization methods. Future research efforts should focus on refining Unicode encoding handling and developing specialized tokenizers that can adeptly navigate the complexities presented by Sandhi and Samas in Hindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741a692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
